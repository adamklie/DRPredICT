GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
/cellar/users/aklie/opt/miniconda3/envs/pytorch_dev2/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py:435: LightningDeprecationWarning: `Accelerator.broadcast` is deprecated in v1.5 and will be removed in v1.6. `Broadcast` logic is implemented directly in the `TrainingTypePlugin` implementations.
  "`Accelerator.broadcast` is deprecated in v1.5 and will be removed in v1.6. "
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Set SLURM handle signals.

  | Name     | Type                 | Params
--------------------------------------------------
0 | encoders | ModuleDict           | 1.2 M 
1 | fcn      | FullyConnectedModule | 16.7 K
--------------------------------------------------
1.2 M     Trainable params
0         Non-trainable params
1.2 M     Total params
4.871     Total estimated model params size (MB)
/cellar/users/aklie/opt/miniconda3/envs/pytorch_dev2/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:433: UserWarning: The number of training samples (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  f"The number of training samples ({self.num_training_batches}) is smaller than the logging interval"
